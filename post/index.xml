<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Justas Dauparas</title>
    <link>https://dauparas.github.io/post/</link>
    <description>Recent content in Posts on Justas Dauparas</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2019 Justas Dauparas</copyright>
    <lastBuildDate>Fri, 29 Mar 2019 16:19:43 -0400</lastBuildDate>
    
	<atom:link href="https://dauparas.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Nonlinear ICA</title>
      <link>https://dauparas.github.io/post/nonlinear_ica/</link>
      <pubDate>Fri, 29 Mar 2019 16:19:43 -0400</pubDate>
      
      <guid>https://dauparas.github.io/post/nonlinear_ica/</guid>
      <description>I have recently got interested in representation learning, especially in disentangled representations of the data. The idea is to find a representation of the high dimensional data like images where the features in the representation space would be &amp;ldquo;disentangled&amp;rdquo;. It is hard to define what is &amp;ldquo;the disentangled representation&amp;rdquo;, but anyway.
I came across this recent paper called &amp;ldquo;Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning&amp;rdquo; and was very curious to try it out.</description>
    </item>
    
    <item>
      <title>Autoregressive Models</title>
      <link>https://dauparas.github.io/post/autoregressive/</link>
      <pubDate>Fri, 29 Mar 2019 12:18:58 -0400</pubDate>
      
      <guid>https://dauparas.github.io/post/autoregressive/</guid>
      <description>Let&amp;rsquo;s talk about Autoregressive Models!</description>
    </item>
    
    <item>
      <title>Unsupervised learning</title>
      <link>https://dauparas.github.io/post/unsupervised_learning/</link>
      <pubDate>Fri, 29 Mar 2019 11:26:20 -0400</pubDate>
      
      <guid>https://dauparas.github.io/post/unsupervised_learning/</guid>
      <description>I will be following the UC Berkeley&amp;rsquo;s CS294-158 Deep Unsupervised Learning course and sharing my short summaries and takeaways.
The course covers Deep Generative Models and Self-supervised Learning with the focus on the theoretical foundations as well as their newly enabled applications. Stay tuned!
Topics:
 Autoregressive Models Lossless Compression Flow Models Latent Variable Models Bits-Back Coding Implicit Models  </description>
    </item>
    
  </channel>
</rss>