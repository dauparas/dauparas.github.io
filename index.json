[{"authors":null,"categories":null,"content":"I am a Postdoctoral Fellow at the Baker Lab which is part of the University of Washington Institute for Protein Design. My research interests lie in the application of mathematics and physics to complex and biological systems. I am particularly interested protein structure prediction and design. ","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://dauparas.github.io/author/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/admin/","section":"author","summary":"I am a Postdoctoral Fellow at the Baker Lab which is part of the University of Washington Institute for Protein Design. My research interests lie in the application of mathematics and physics to complex and biological systems. I am particularly interested protein structure prediction and design. ","tags":null,"title":"Justas Dauparas","type":"author"},{"authors":[],"categories":[],"content":" This is a short list of books and lectures with links for myself and anyone else who might find them useful. A book is worth a thousand arXiv papers!\nMachine Learning Books  Pattern Recognition and Machine Learning by Christopher M. Bishop, Information Theory, Inference, and Learning Algorithms by David J.C. MacKay, videos, Machine Learning: A Probabilistic Perspective by Kevin P. Murphy, Bayesian Reasoning and Machine Learning by David Barber, Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, The Elements of Statistical Learning by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, All of Statistics A Concise Course in Statistical Inference by Larry Wasserman, Reinforcement learning: An introduction by Richard S. Sutton and Andrew G. Barto.  Lectures on Machine Learning  fast.ai, Natural Language Processing with Deep Learning, CS224n at Stanford, Convolutional Neural Networks for Visual Recognition CS231n at Stanford, Deep Unsupervised Learning, CS294-158 at UC Berkeley, Deep Reinforcement Learning CS285 at UC Berkeley.  Lectures on Theoretical Physics by David Tong  Dynamics and Relativity, Classical Dynamics, Electromagnetism, Topics in Quantum Mechanics, Solid State Physics, Quantum Hall Effect, Statistical Physics, Kinetic Theory, Statistical Field Theory, Quantum Field Theory, Gauge Theory, Solitons, General Relativity, Cosmology, String Theory.  ","date":1595760286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595760286,"objectID":"c1656170721cff2256f5e3a26b3ef485","permalink":"https://dauparas.github.io/post/learning/","publishdate":"2020-07-26T06:44:46-04:00","relpermalink":"/post/learning/","section":"post","summary":"Some books and online lectures on machine learning!","tags":[],"title":"Learn it yourself - online resources","type":"post"},{"authors":[],"categories":[],"content":"In progress\u0026hellip;\nDefinitions\n A topological space $M$ is a set of points, endowed with a topology $\\mathcal{T}$. This is a collection of open subsets $\\{\\mathcal{O}_{\\alpha}\\subset M\\}$ which obey: \\begin{align} \u0026amp;M\\in\\mathcal{T}, \\emptyset\\in\\mathcal{T},\\\\\n\u0026amp;\\mathcal{O}_1\\in\\mathcal{T}, \\mathcal{O}_2\\in\\mathcal{T}\\implies \\mathcal{O}_1\\cap\\mathcal{O}_2\\in\\mathcal{T},\\\\\n\u0026amp;\\mathcal{O}_{\\gamma}\\in\\mathcal{T}\\implies \\cup_{\\gamma}\\mathcal{O}_{\\gamma}\\in\\mathcal{T}. \\end{align}\n A homeomorphism between topological spaces $(M, \\mathcal{T})$ and $(\\tilde{M}, \\tilde{\\mathcal{T}})$ is a map $f:M\\mapsto\\tilde{M}$ which is\n Bijective The function and its inverse are continuous. We say that $f$ is continuous if, for all $\\tilde{\\mathcal{O}}\\in\\tilde{\\mathcal{T}}, f^{-1}(\\tilde{\\mathcal{O}})\\in\\mathcal{T}$.  An n-dimensional differentiable manifold is a Hausdorff topological space $M$ such that\n $M$ is locally homeomorphic to $\\mathbf{R}^{n}$. Take two open subsets $\\mathcal{O}_{\\alpha}$ and $\\mathcal{O}_{\\beta}$ that overlap, so that $\\mathcal{O}_{\\alpha}\\cap \\mathcal{O}_{\\beta}\\neq\\emptyset$. We require that the corresponding maps $\\phi_{\\alpha}:\\mathcal{O}_{\\alpha}\\rightarrow U_{\\alpha}$ and $\\phi_{\\beta}:\\mathcal{O}_{\\beta}\\rightarrow U_{\\beta}$ are compatible, meaning that the map $\\phi_{\\alpha}\\circ \\phi_{\\beta}^{-1}: \\phi_{\\beta}(\\mathcal{O}_{\\alpha}\\cap \\mathcal{O}_{\\beta})\\rightarrow \\phi_{\\alpha}(\\mathcal{O}_{\\alpha}\\cap \\mathcal{O}_{\\beta})$ is infinitely differentiable.   The maps $\\phi_{\\alpha}$ are called charts and the collection of charts is called an atlas. You can think of each chart as providing a coordinate system to label the region $\\mathcal{O}_{\\alpha}$ of $M$. The coordinate associated to $p\\in\\mathcal{O}_{\\alpha}$ is \\begin{align} \\phi_{\\alpha}(p)=(x^{1}(p),\u0026hellip;,x^{n}(p)) \\end{align} We write the coordinate is shorthand as simply $x^{\\mu}(p)$, with $\\mu=1,\u0026hellip;,n$. The maps $\\phi_{\\alpha}\\circ \\phi_{\\beta}^{-1}$ take us between different coordinate systems and are called transition functions. The compatibility condition is there to ensure that there is no inconsistency between these different coordinate systems.\nThe advantage of locally mapping a manifold to $\\mathbf{R}^n$ is that we can now import our knowledge of how to do maths on $\\mathbf{R}^n$.\nWe say that a function $f:M\\rightarrow\\mathbf{R}$ is smooth, if the map $f\\circ \\phi^{-1}: U\\rightarrow \\mathbf{R}$ is smooth for all charts $\\phi$.\nSimilarly, we say that a map $f:M\\rightarrow N$ between two manifolds $M$ and $N$ is smooth if the map $\\phi\\circ f \\circ \\phi^{-1}:U\\rightarrow V$ is smooth for all charts $\\phi: M\\rightarrow U\\subset\\mathbf{R}^{dim(M)}$ and $\\psi: N\\rightarrow V\\subset\\mathbf{R}^{dim(N)}$\nA diffeomorphism is defined to be a smooth homeomorphism $f:M\\rightarrow N$. In other words it is an invertible, smooth map between manifolds $M$ and $N$ that has a smooth inverse. If such a diffeomorphism exists then the manifolds $M$ and $N$ are said to be diffeomorphic. The existence of an inverse means $M$ and $N$ necessarily have the same dimension.\nTangent Spaces\nConsider a function $f:M\\rightarrow\\mathbf{R}$. To differentiate the function at some point $p$, we introduce a chart $\\phi=(x^{1},\u0026hellip;,x^{n})$ in a neighbourhood of $p$. We can then construct the map $f\\circ \\phi^{-1}:U\\rightarrow\\mathbf{R}$ with $U\\subset\\mathbf{R}^{n}$. But we know how to differentiate functions on $\\mathbf{R}^{n}$ and this gives us a way to differentiate functions on $M$, namely\n\\begin{align} \\left.\\frac{\\partial f}{\\partial x^{\\mu}}\\right|_{p}:=\\left.\\frac{\\partial\\left(f \\circ \\phi^{-1}\\right)}{\\partial x^{\\mu}}\\right|_{\\phi(p)} \\end{align}\nClearly this depends on the choice of chart $\\phi$ and coordinates $x^{\\mu}$. We would like to give a coordinate independent definition of differentiation, and then understand what happens when we choose to describe this object using different coordinates.\nTangent Vectors\nWe will consider smooth functions over a manifold $M$. We denote the set of all smooth functions as $C^{\\infty}(M)$.\nA tangent vector $X_{p}$ is an object that differentiates functions at a point $p\\in M$. Specifically, $X_p:C^{\\infty}(M)\\rightarrow \\mathbf{R}$ satisfying\n Linearity: $X_{p}(f+g)=X_{p}(f)+X_{p}(g)$ for all $f,g\\in C^{\\infty}(M)$. $X_{p}(f)=0$ when $f$ is the constant function. Leibnizarity: $X_{p}(fg)=f(p)X_{p}(g)+X_{p}(f)g(p)$ for all $f,g\\in C^{\\infty}(M)$. This is a product rule.  Theorem:\nThe set of all tangent vectors at point $p$ forms an n-dimensional vector space. We call this space the tangent space $T_{p}(M)$. The tangent vectors $\\left.\\partial_{\\mu}\\right|_{p}$ provide a basis for $T_{p}(M)$. This means that we can write any tangent vector as\n\\begin{align} X_{p}=X^{\\mu}\\left.\\partial_{\\mu}\\right|_{p} \\end{align}\nwith $X^{\\mu}=X_{p}(x^{\\mu})$ the components of the tangent vector in this basis.\n","date":1587811486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587811486,"objectID":"74402fbc556a9c62c9771e6ebf90d2b6","permalink":"https://dauparas.github.io/post/diff_geometry/","publishdate":"2020-04-25T06:44:46-04:00","relpermalink":"/post/diff_geometry/","section":"post","summary":"Introduction to differential geometry","tags":[],"title":"Differential Geometry","type":"post"},{"authors":[],"categories":[],"content":" In progress\u0026hellip;\nLearn about normalizing flows  Review paper: Normalizing Flows for Probabilistic Modeling and Inference, Papamakarios, Nalisnick et al. Review paper: Normalizing Flows: An Introduction and Review of Current Methods, Kobyzev et al. Video lecture: Flow Models \u0026ndash; CS294-158-SP20 Deep Unsupervised Learning Video lecture: Invertible Models and Normalizing Flows, Laurent Dinh, ICLR 2020 Video lecture: Tutorial on normalizing flows, Eric Jang, ICML 2019 Blog: Flow-based Deep Generative Models, Lilian Weng Blog: Normalizing Flows Tutorial, Eric Jang  Definitions\n $p_{x}^{*}(\\mathbf{x})=p_{x}^{*}(x_1, x_2,\u0026hellip;, x_D) \\sim \\mathbf{x}\\in\\mathbb{R}^{D}$, unknown true distribution of data, $p_{x}(\\mathbf{x})=p_{x}(x_1, x_2,\u0026hellip;, x_D) \\sim \\mathbf{x}\\in\\mathbb{R}^{D}$, model distribution of data, $p_{u}(\\mathbf{u})=p_{u}(u_1, u_2,\u0026hellip;, u_D) \\sim \\mathbf{u}\\in\\mathbb{R}^{D}$, simple (normal) distribution, $T(\\mathbf{u})=\\mathbf{x}\\in\\mathbb{R}^{D}$, transformation function, $T^{-1}(\\mathbf{x})=\\mathbf{u}\\in\\mathbb{R}^{D}$, inverse transformation function, $J_{T}(\\mathbf{u})=\\frac{\\partial}{\\partial \\mathbf{u}}T(\\mathbf{u})=\\frac{\\partial \\mathbf{x}}{\\partial \\mathbf{u}}\\in\\mathbb{R}^{D\\times D}$, Jacobian matrix of transformation function, $J_{T^{-1}}(\\mathbf{x})=\\frac{\\partial}{\\partial \\mathbf{x}}T^{-1}(\\mathbf{x})=\\frac{\\partial \\mathbf{u}}{\\partial \\mathbf{x}}\\in\\mathbb{R}^{D\\times D}$, Jacobian matrix of inverse transformation function,    Figure 1. A differentiable transformation $T$ maps a D-dimentional vector $\\mathbf{u}\\in\\mathbb{R}^{D}$ to a D-dimentional vector $\\mathbf{x}\\in\\mathbb{R}^{D}$ via yellow arrow. This transformation has a differentiable inverse denoted by $T^{-1}$ that maps $\\mathbf{x}$ to $\\mathbf{u}$ via purple arrow. Jacobian matrices for these functions are depicted as squares, $J_{T}(\\mathbf{u})\\in\\mathbb{R}^{D\\times D}$ and $J_{T^{-1}}(\\mathbf{x})\\in\\mathbb{R}^{D\\times D}$.   Change of variables\nWe can express probability density $p_{x}$ in terms of $p_{u}$ using change of variables:\n $p_{x}(\\mathbf{x})=p_{u}(\\mathbf{u})\\left|\\frac{\\partial\\mathbf{u}}{\\partial\\mathbf{x}}\\right|=p_{u}(\\mathbf{u})\\left|\\frac{\\partial}{\\partial\\mathbf{x}} T^{-1}(\\mathbf{x})\\right|=p_{u}(T^{-1}(\\mathbf{x}))\\left|\\det{J_{T^{-1}}(\\mathbf{x})}\\right|$ $p_{x}(\\mathbf{x})=p_{u}(\\mathbf{u})\\left|\\frac{\\partial\\mathbf{x}}{\\partial\\mathbf{u}}\\right|^{-1}=p_{u}(\\mathbf{u})\\left|\\frac{\\partial}{\\partial\\mathbf{u}}T(\\mathbf{u})\\right|^{-1}=p_{u}(\\mathbf{u})\\left|\\det{J_{T}(\\mathbf{u})}\\right|^{-1}$  given that the transformation $T$ is invertible and both $T$ and $T^{-1}$ are differentiable.\nApplications\n Sample from $p_{x}(\\mathbf{x})$ by sampling points from $p_{u}(\\mathbf{u})$, i.e. $\\mathbf{x}=T(\\mathbf{u})$ where $\\mathbf{u} \\sim p_{u}(\\mathbf{u})$. This requires the ability to sample from $p_{u}(\\mathbf{u})$ and to compute the transformation $T$. Evaluate model\u0026rsquo;s density using $p_{x}(\\mathbf{x})=p_{u}(T^{-1}(\\mathbf{x}))\\left|\\det{J_{T^{-1}}(\\mathbf{x})}\\right|$. This requires computing the inverse transformation $T^{-1}$ and its Jacobian determinant, and evaluating the density $p_{u}(\\mathbf{u})$.  Function composition\n  Figure 2. A differentiable transformation $T=T_3\\circ T_2\\circ T_1$ which is a compostion of three functions $T_1, T_2, T_3$ maps a vector $\\mathbf{u}\\in\\mathbb{R}^{D}$ to a vector $\\mathbf{x}\\in\\mathbb{R}^{D}$ via yellow arrow. The inverse transformation $T^{-1}=T_{1}^{-1}\\circ T_{2}^{-1}\\circ T_{3}^{-1}$ maps $\\mathbf{u}$ to $\\mathbf{x}$ via purple arrow. $\\mathbf{z}_{i}$ vectors denote in between states with $\\mathbf{u}=\\mathbf{z}_{0}$ and $\\mathbf{x}=\\mathbf{z}_{3}$.   Given three invertible and differentiable transformations $T_1, T_2$ and $T_3$ their composition $T=T_3 \\circ T_2 \\circ T_1$ is also invertible and differentiable:\n\\begin{align} T(\\mathbf{u})\u0026amp;=T_3 \\circ T_2 \\circ T_1 (\\mathbf{u}),\\\\\nT^{-1}(\\mathbf{x})\u0026amp;=(T_3 \\circ T_2 \\circ T_1)^{-1}(\\mathbf{x}) = T_1^{-1} \\circ T_2^{-1} \\circ T_3^{-1}(\\mathbf{x}),\\\\\n\\frac{\\partial}{\\partial \\mathbf{u}}T(\\mathbf{u})\u0026amp;=\\frac{\\partial}{\\partial \\mathbf{u}}T_3 \\circ T_2 \\circ T_1 (\\mathbf{u})=\\frac{\\partial T_1(\\mathbf{u})}{\\partial \\mathbf{u}}\\frac{\\partial T_2(T_1(\\mathbf{u}))}{\\partial T_1(\\mathbf{u})}\\frac{\\partial T_3(T_2(T_1(\\mathbf{u})))}{\\partial T_2(T_1(\\mathbf{u}))}=\\frac{\\partial T_1}{\\partial \\mathbf{u}}\\frac{\\partial T_2}{\\partial T_1}\\frac{\\partial T_3}{\\partial T_2},\\\\\n\\frac{\\partial}{\\partial \\mathbf{x}}T^{-1}(\\mathbf{x})\u0026amp;=\\frac{\\partial}{\\partial \\mathbf{x}}T_1^{-1} \\circ T_2^{-1} \\circ T_3^{-1} (\\mathbf{x})=\\frac{\\partial T_3^{-1}(\\mathbf{x})}{\\partial \\mathbf{x}}\\frac{\\partial T_2^{-1}(T_3^{-1}(\\mathbf{x}))}{\\partial T_3^{-1}(\\mathbf{x})}\\frac{\\partial T_1^{-1}(T_2^{-1}(T_3^{-1}(\\mathbf{x})))}{\\partial T_2^{-1}(T_3^{-1}(\\mathbf{x}))}=\\frac{\\partial T_3^{-1}}{\\partial \\mathbf{x}}\\frac{\\partial T_2^{-1}}{\\partial T_3^{-1}}\\frac{\\partial T_1^{-1}}{\\partial T_2^{-1}}, \\end{align} where for differentiation the chain rule was applied. Now to calculate the Jacobian matrix of this transformation we can use the fact that the determinant of a matrix product of square matrices equals the product of their determinants, i.e. $\\det{AB}=\\det{A}\\cdot\\det{B}$ where $A, B\\in\\mathbb{R}^{D\\times D}$.\n\\begin{align} \\det{\\frac{\\partial}{\\partial \\mathbf{u}}T(\\mathbf{u})}=\u0026amp;\\det{J_{T_3\\circ T_2 \\circ T_1}(\\mathbf{u})}=\\det{\\frac{\\partial T_1}{\\partial \\mathbf{u}}\\frac{\\partial T_2}{\\partial T_1}\\frac{\\partial T_3}{\\partial T_2}}=\\det{J_{T_{1}}(\\mathbf{u})}\\cdot\\det{J_{T_2}(T_{1}(\\mathbf{u}))}\\cdot\\det{J_{T_3}(T_{2}(T_{1}(\\mathbf{u})))},\\\\\n\\det{\\frac{\\partial}{\\partial \\mathbf{x}}T^{-1}(\\mathbf{x})}\u0026amp;=\\det{J_{T_1^{-1} \\circ T_2^{-1}\\circ T_3^{-1}}(\\mathbf{x})}=\\det{\\frac{\\partial T_3^{-1}}{\\partial \\mathbf{x}}\\frac{\\partial T_2^{-1}}{\\partial T_3^{-1}}\\frac{\\partial T_1^{-1}}{\\partial T_2^{-1}}}=\\det{J_{T_{3}^{-1}}(\\mathbf{x})}\\cdot\\det{J_{T_2^{-1}}(T_{3}^{-1}(\\mathbf{x}))}\\cdot\\det{J_{T_1^{-1}}(T_2^{-1}(T_{3}^{-1}(\\mathbf{x})))}. \\end{align}\nMinimizing loss of log likelihood \\begin{align} \\mathcal{L}(\\boldsymbol{\\theta})\u0026amp;=-\\mathbb{E}_{p_{x}^{*}(\\mathbf{x})}\\left[\\log{p_{x}(\\mathbf{x};\\boldsymbol{\\theta})}\\right]\\\\\n\u0026amp;=-\\mathbb{E}_{p_{x}^{*}(\\mathbf{x})}\\left[\\log{p_{u}(T^{-1}(\\mathbf{x}; \\boldsymbol{\\phi});\\boldsymbol{\\psi})}+\\log{\\left|\\det{J_{T^{-1}}(\\mathbf{x}; \\boldsymbol{\\phi})}\\right|}\\right]\\\\\n\u0026amp;\\approx-\\frac{1}{N}\\sum_{n=1}^{N}\\log{p_{u}(T^{-1}(\\mathbf{x}_{n}; \\boldsymbol{\\phi});\\boldsymbol{\\psi})}+\\log{\\left|\\det{J_{T^{-1}}(\\mathbf{x}_{n}; \\boldsymbol{\\phi})}\\right|} \\end{align}\n","date":1587811486,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587811486,"objectID":"f18e601a168bd99e5d7ba90fd193cbb3","permalink":"https://dauparas.github.io/post/normalizing_flows/","publishdate":"2020-04-25T06:44:46-04:00","relpermalink":"/post/normalizing_flows/","section":"post","summary":"Review of normalizing flows paper","tags":[],"title":"Normalizing Flows","type":"post"},{"authors":null,"categories":[],"content":"This is a link to theoretical physics.\n","date":1553873180,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553873180,"objectID":"ec8dcaf41bb54c4d82ded5037bc525c7","permalink":"https://dauparas.github.io/links/physics/","publishdate":"2019-03-29T11:26:20-04:00","relpermalink":"/links/physics/","section":"links","summary":"This is a link to theoretical physics.","tags":[],"title":"Theoretical Physics","type":"links"},{"authors":["Justas Dauparas","Haobo Wang","Peter Koo","Mor Nitzan","Sergey Ovchinnikov"],"categories":null,"content":"","date":1551661279,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551661279,"objectID":"6f3f622cfd3e848ed35db71e052c9b46","permalink":"https://dauparas.github.io/publication/unified/","publishdate":"2019-03-03T20:01:19-05:00","relpermalink":"/publication/unified/","section":"publication","summary":"Submitted to the ICML Workshop on Computational Biology.","tags":[],"title":"Unified framework for multivariate distributions in biological sequences","type":"publication"},{"authors":["Justas Dauparas","Katja Hofmann","Ryota Tomioka"],"categories":null,"content":"","date":1527639497,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527639497,"objectID":"6e6cd07e02f75752405666188a577387","permalink":"https://dauparas.github.io/publication/rl/","publishdate":"2018-05-29T19:18:17-05:00","relpermalink":"/publication/rl/","section":"publication","summary":"We show that Q-learning with nonlinear Q-function and no explicit exploration can learn several standard benchmark tasks.","tags":[],"title":"Depth and nonlinearity induce implicit exploration","type":"publication"},{"authors":["Xu Haoran","Justas Dauparas","Debasish Das","Eric Lauga","Yilin Wu"],"categories":null,"content":"","date":1519521906,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519521906,"objectID":"f8c0251014175e98e3f949ddc3e3938d","permalink":"https://dauparas.github.io/publication/self_organisation/","publishdate":"2018-02-24T20:25:06-05:00","relpermalink":"/publication/self_organisation/","section":"publication","summary":"We discovered that motile cells in sessile colonies of peritrichously flagellated bacteria can self-organize into motile bands that can drive long-range fluid transport at a constant speed of ~30 μm/s, providing a stable high-speed avenue for material transport at the colony scale.","tags":[],"title":"Self-organization of swimmers drives long-range fluid transport in bacterial colonies","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"62e3e13b516536879ad3e2adb5ab40ef","permalink":"https://dauparas.github.io/publication/corner_flows/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/corner_flows/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"cf3bfd228ce70b343a4995f2efd4d5de","permalink":"https://dauparas.github.io/publication/flagellar_flows/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/flagellar_flows/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"24892bd37ce9f61c7521bbb535f7c432","permalink":"https://dauparas.github.io/publication/helical_micropumps/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/helical_micropumps/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"0f40cf545adf79432be77d9037b7794b","permalink":"https://dauparas.github.io/publication/thesis/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/publication/thesis/","section":"publication","summary":"","tags":null,"title":"","type":"publication"},{"authors":null,"categories":[],"content":"This is a link to theoretical physics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f203a35d55569061480d498c648abd9a","permalink":"https://dauparas.github.io/talk/physics/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talk/physics/","section":"talk","summary":"This is a link to theoretical physics.","tags":[],"title":"Theoretical Physics","type":"talk"}]