<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Justas Dauparas on Justas Dauparas</title>
    <link>https://dauparas.github.io/</link>
    <description>Recent content in Justas Dauparas on Justas Dauparas</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2020 Justas Dauparas</copyright>
    <lastBuildDate>Sun, 26 Jul 2020 06:44:46 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Learn it yourself - online resources</title>
      <link>https://dauparas.github.io/post/learning/</link>
      <pubDate>Sun, 26 Jul 2020 06:44:46 -0400</pubDate>
      
      <guid>https://dauparas.github.io/post/learning/</guid>
      <description>

&lt;p&gt;This is a short list of books and lectures with links for myself and anyone else who might find them useful. A book is worth a thousand arXiv papers!&lt;/p&gt;

&lt;h3 id=&#34;machine-learning-books&#34;&gt;Machine Learning Books&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf&#34; target=&#34;_blank&#34;&gt;Pattern Recognition and Machine Learning&lt;/a&gt; by Christopher M. Bishop,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.inference.org.uk/itprnn/book.pdf&#34; target=&#34;_blank&#34;&gt;Information Theory, Inference, and Learning Algorithms&lt;/a&gt; by David J.C. MacKay, &lt;a href=&#34;https://www.youtube.com/watch?v=BCiZc0n6COY&amp;amp;list=PLruBu5BI5n4aFpG32iMbdWoRVAA-Vcso6&#34; target=&#34;_blank&#34;&gt;videos&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://noiselab.ucsd.edu/ECE228/Murphy_Machine_Learning.pdf&#34; target=&#34;_blank&#34;&gt;Machine Learning: A Probabilistic Perspective&lt;/a&gt; by Kevin P. Murphy,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web4.cs.ucl.ac.uk/staff/D.Barber/pmwiki/pmwiki.php?n=Brml.Online&#34; target=&#34;_blank&#34;&gt;Bayesian Reasoning and Machine Learning&lt;/a&gt; by David Barber,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.deeplearningbook.org/&#34; target=&#34;_blank&#34;&gt;Deep Learning&lt;/a&gt; by Ian Goodfellow, Yoshua Bengio, and Aaron Courville,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://web.stanford.edu/~hastie/ElemStatLearn/printings/ESLII_print12.pdf&#34; target=&#34;_blank&#34;&gt;The Elements of Statistical Learning&lt;/a&gt; by Trevor Hastie, Robert Tibshirani, and Jerome Friedman,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.stat.cmu.edu/~larry/all-of-statistics/&#34; target=&#34;_blank&#34;&gt;All of Statistics A Concise Course in Statistical Inference&lt;/a&gt; by Larry Wasserman,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.incompleteideas.net/book/RLbook2018.pdf&#34; target=&#34;_blank&#34;&gt;Reinforcement learning: An introduction&lt;/a&gt; by Richard S. Sutton and Andrew G. Barto.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;lectures-on-machine-learning&#34;&gt;Lectures on Machine Learning&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.fast.ai/&#34; target=&#34;_blank&#34;&gt;fast.ai&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://web.stanford.edu/class/cs224n/index.html#schedule&#34; target=&#34;_blank&#34;&gt;Natural Language Processing with Deep Learning&lt;/a&gt;, CS224n at Stanford,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://vision.stanford.edu/teaching/cs231n/&#34; target=&#34;_blank&#34;&gt;Convolutional Neural Networks for Visual Recognition&lt;/a&gt; CS231n at Stanford,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://sites.google.com/view/berkeley-cs294-158-sp20/home&#34; target=&#34;_blank&#34;&gt;Deep Unsupervised Learning&lt;/a&gt;, CS294-158 at UC Berkeley,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/&#34; target=&#34;_blank&#34;&gt;Deep Reinforcement Learning&lt;/a&gt; CS285 at UC Berkeley.&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- ### Computer Science Lectures
1. [Structure and Interpretation of Computer Programs] (https://web.mit.edu/alexmv/6.037/sicp.pdf) by

### Biology Lectures
1. [Introduction to Biology - The Secret of Life](https://www.edx.org/course/introduction-to-biology-the-secret-of-life-3), --&gt;

&lt;h3 id=&#34;lectures-on-theoretical-physics-by-david-tong&#34;&gt;Lectures on Theoretical Physics by David Tong&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/relativity.html&#34; target=&#34;_blank&#34;&gt;Dynamics and Relativity&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/dynamics.html&#34; target=&#34;_blank&#34;&gt;Classical Dynamics&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/em.html&#34; target=&#34;_blank&#34;&gt;Electromagnetism&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/topicsinqm.html&#34; target=&#34;_blank&#34;&gt;Topics in Quantum Mechanics&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/solidstate.html&#34; target=&#34;_blank&#34;&gt;Solid State Physics&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/qhe.html&#34; target=&#34;_blank&#34;&gt;Quantum Hall Effect&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/statphys.html&#34; target=&#34;_blank&#34;&gt;Statistical Physics&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/kinetic.html&#34; target=&#34;_blank&#34;&gt;Kinetic Theory&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/sft.html&#34; target=&#34;_blank&#34;&gt;Statistical Field Theory&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/qft.html&#34; target=&#34;_blank&#34;&gt;Quantum Field Theory&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/gaugetheory.html&#34; target=&#34;_blank&#34;&gt;Gauge Theory&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/tasi.html&#34; target=&#34;_blank&#34;&gt;Solitons&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/gr.html&#34; target=&#34;_blank&#34;&gt;General Relativity&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/cosmo.html&#34; target=&#34;_blank&#34;&gt;Cosmology&lt;/a&gt;,&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://www.damtp.cam.ac.uk/user/tong/string.html&#34; target=&#34;_blank&#34;&gt;String Theory&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Differential Geometry</title>
      <link>https://dauparas.github.io/post/diff_geometry/</link>
      <pubDate>Sat, 25 Apr 2020 06:44:46 -0400</pubDate>
      
      <guid>https://dauparas.github.io/post/diff_geometry/</guid>
      <description>&lt;p&gt;In progress&amp;hellip;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;A topological space $M$ is a set of points, endowed with a topology $\mathcal{T}$. This is a collection of open subsets $\{\mathcal{O}_{\alpha}\subset M\}$ which obey:
\begin{align}
&amp;amp;M\in\mathcal{T}, \emptyset\in\mathcal{T},\\&lt;br /&gt;
&amp;amp;\mathcal{O}_1\in\mathcal{T}, \mathcal{O}_2\in\mathcal{T}\implies \mathcal{O}_1\cap\mathcal{O}_2\in\mathcal{T},\\&lt;br /&gt;
&amp;amp;\mathcal{O}_{\gamma}\in\mathcal{T}\implies \cup_{\gamma}\mathcal{O}_{\gamma}\in\mathcal{T}.
\end{align}&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;A homeomorphism between topological spaces $(M, \mathcal{T})$ and $(\tilde{M}, \tilde{\mathcal{T}})$ is a map $f:M\mapsto\tilde{M}$ which is&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Bijective&lt;/li&gt;
&lt;li&gt;The function and its inverse are continuous. We say that $f$ is continuous if, for all $\tilde{\mathcal{O}}\in\tilde{\mathcal{T}}, f^{-1}(\tilde{\mathcal{O}})\in\mathcal{T}$.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;An n-dimensional differentiable manifold is a Hausdorff topological space $M$ such that&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$M$ is locally homeomorphic to $\mathbf{R}^{n}$.&lt;/li&gt;
&lt;li&gt;Take two open subsets $\mathcal{O}_{\alpha}$ and $\mathcal{O}_{\beta}$ that overlap, so that $\mathcal{O}_{\alpha}\cap \mathcal{O}_{\beta}\neq\emptyset$. We require that the corresponding maps $\phi_{\alpha}:\mathcal{O}_{\alpha}\rightarrow U_{\alpha}$ and $\phi_{\beta}:\mathcal{O}_{\beta}\rightarrow U_{\beta}$ are compatible, meaning that the map $\phi_{\alpha}\circ \phi_{\beta}^{-1}: \phi_{\beta}(\mathcal{O}_{\alpha}\cap \mathcal{O}_{\beta})\rightarrow \phi_{\alpha}(\mathcal{O}_{\alpha}\cap \mathcal{O}_{\beta})$ is infinitely differentiable.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The maps $\phi_{\alpha}$ are called charts and the collection of charts is called an atlas. You can think of each chart as providing a coordinate system to label the region $\mathcal{O}_{\alpha}$ of $M$. The coordinate associated to $p\in\mathcal{O}_{\alpha}$ is
\begin{align}
\phi_{\alpha}(p)=(x^{1}(p),&amp;hellip;,x^{n}(p))
\end{align}
We write the coordinate is shorthand as simply $x^{\mu}(p)$, with $\mu=1,&amp;hellip;,n$. The maps $\phi_{\alpha}\circ \phi_{\beta}^{-1}$ take us between different coordinate systems and are called &lt;i&gt;transition functions&lt;/i&gt;. The compatibility condition is there to ensure that there is no
inconsistency between these different coordinate systems.&lt;/p&gt;

&lt;p&gt;The advantage of locally mapping a manifold to $\mathbf{R}^n$ is that we can now import our
knowledge of how to do maths on $\mathbf{R}^n$.&lt;/p&gt;

&lt;p&gt;We say that a function $f:M\rightarrow\mathbf{R}$ is smooth, if the map $f\circ \phi^{-1}: U\rightarrow \mathbf{R}$ is smooth for all charts $\phi$.&lt;/p&gt;

&lt;p&gt;Similarly, we say that a map $f:M\rightarrow N$ between two manifolds $M$ and $N$ is smooth if the map $\phi\circ f \circ \phi^{-1}:U\rightarrow V$ is smooth for all charts $\phi: M\rightarrow U\subset\mathbf{R}^{dim(M)}$ and $\psi: N\rightarrow V\subset\mathbf{R}^{dim(N)}$&lt;/p&gt;

&lt;p&gt;A &lt;i&gt;diffeomorphism&lt;/i&gt; is defined to be a smooth homeomorphism $f:M\rightarrow N$. In other
words it is an invertible, smooth map between manifolds $M$ and $N$ that has a smooth inverse. If such a diffeomorphism exists then the manifolds $M$ and $N$ are said to be diffeomorphic. The existence of an inverse means $M$ and $N$ necessarily have the same dimension.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tangent Spaces&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Consider a function $f:M\rightarrow\mathbf{R}$. To differentiate the function at some point $p$, we introduce a chart $\phi=(x^{1},&amp;hellip;,x^{n})$ in a neighbourhood of $p$. We can then construct the
map $f\circ \phi^{-1}:U\rightarrow\mathbf{R}$ with $U\subset\mathbf{R}^{n}$.  But we know how to differentiate functions on $\mathbf{R}^{n}$ and this gives us a way to differentiate functions on $M$, namely&lt;/p&gt;

&lt;p&gt;\begin{align}
\left.\frac{\partial f}{\partial x^{\mu}}\right|_{p}:=\left.\frac{\partial\left(f \circ \phi^{-1}\right)}{\partial x^{\mu}}\right|_{\phi(p)}
\end{align}&lt;/p&gt;

&lt;p&gt;Clearly this depends on the choice of chart $\phi$ and coordinates $x^{\mu}$. We would like to
give a coordinate independent definition of differentiation, and then understand what
happens when we choose to describe this object using different coordinates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tangent Vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We will consider smooth functions over a manifold $M$. We denote the set of all smooth
functions as $C^{\infty}(M)$.&lt;/p&gt;

&lt;p&gt;A &lt;i&gt;tangent vector&lt;/i&gt; $X_{p}$ is an object that differentiates functions at a point $p\in M$. Specifically, $X_p:C^{\infty}(M)\rightarrow \mathbf{R}$ satisfying&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Linearity: $X_{p}(f+g)=X_{p}(f)+X_{p}(g)$ for all $f,g\in C^{\infty}(M)$.&lt;/li&gt;
&lt;li&gt;$X_{p}(f)=0$ when $f$ is the constant function.&lt;/li&gt;
&lt;li&gt;Leibnizarity: $X_{p}(fg)=f(p)X_{p}(g)+X_{p}(f)g(p)$ for all $f,g\in C^{\infty}(M)$. This is a product rule.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Theorem:&lt;/p&gt;

&lt;p&gt;The set of all tangent vectors at point $p$ forms an n-dimensional vector space. We call this space the &lt;i&gt;tangent space&lt;/i&gt; $T_{p}(M)$. The tangent vectors $\left.\partial_{\mu}\right|_{p}$ provide a basis for $T_{p}(M)$. This means that we can write any tangent vector as&lt;/p&gt;

&lt;p&gt;\begin{align}
X_{p}=X^{\mu}\left.\partial_{\mu}\right|_{p}
\end{align}&lt;/p&gt;

&lt;p&gt;with $X^{\mu}=X_{p}(x^{\mu})$ the components of the tangent vector in this basis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Normalizing Flows</title>
      <link>https://dauparas.github.io/post/normalizing_flows/</link>
      <pubDate>Sat, 25 Apr 2020 06:44:46 -0400</pubDate>
      
      <guid>https://dauparas.github.io/post/normalizing_flows/</guid>
      <description>

&lt;p&gt;In progress&amp;hellip;&lt;/p&gt;

&lt;h3 id=&#34;learn-about-normalizing-flows&#34;&gt;Learn about normalizing flows&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1912.02762.pdf&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Review paper&lt;/strong&gt;: Normalizing Flows for Probabilistic Modeling and Inference, Papamakarios, Nalisnick et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/pdf/1908.09257.pdf&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Review paper&lt;/strong&gt;: Normalizing Flows: An Introduction and Review of Current Methods, Kobyzev et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=JBb5sSC0JoY&amp;amp;feature=youtu.be&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Video lecture&lt;/strong&gt;: Flow Models &amp;ndash; CS294-158-SP20 Deep Unsupervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://iclr.cc/virtual_2020/speaker_4.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Video lecture&lt;/strong&gt;: Invertible Models and Normalizing Flows, Laurent Dinh, ICLR 2020&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://slideslive.com/38917907/tutorial-on-normalizing-flows&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Video lecture&lt;/strong&gt;: Tutorial on normalizing flows, Eric Jang, ICML 2019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Blog&lt;/strong&gt;: Flow-based Deep Generative Models, Lilian Weng&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.evjang.com/2018/01/nf1.html&#34; target=&#34;_blank&#34;&gt;&lt;strong&gt;Blog&lt;/strong&gt;: Normalizing Flows Tutorial, Eric Jang&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Definitions&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$p_{x}^{*}(\mathbf{x})=p_{x}^{*}(x_1, x_2,&amp;hellip;, x_D) \sim \mathbf{x}\in\mathbb{R}^{D}$, unknown true distribution of data,&lt;/li&gt;
&lt;li&gt;$p_{x}(\mathbf{x})=p_{x}(x_1, x_2,&amp;hellip;, x_D) \sim \mathbf{x}\in\mathbb{R}^{D}$, model distribution of data,&lt;/li&gt;
&lt;li&gt;$p_{u}(\mathbf{u})=p_{u}(u_1, u_2,&amp;hellip;, u_D) \sim \mathbf{u}\in\mathbb{R}^{D}$, simple (normal) distribution,&lt;/li&gt;
&lt;li&gt;$T(\mathbf{u})=\mathbf{x}\in\mathbb{R}^{D}$, transformation function,&lt;/li&gt;
&lt;li&gt;$T^{-1}(\mathbf{x})=\mathbf{u}\in\mathbb{R}^{D}$, inverse transformation function,&lt;/li&gt;
&lt;li&gt;$J_{T}(\mathbf{u})=\frac{\partial}{\partial \mathbf{u}}T(\mathbf{u})=\frac{\partial \mathbf{x}}{\partial \mathbf{u}}\in\mathbb{R}^{D\times D}$, Jacobian matrix of transformation function,&lt;/li&gt;
&lt;li&gt;$J_{T^{-1}}(\mathbf{x})=\frac{\partial}{\partial \mathbf{x}}T^{-1}(\mathbf{x})=\frac{\partial \mathbf{u}}{\partial \mathbf{x}}\in\mathbb{R}^{D\times D}$, Jacobian matrix of inverse transformation function,&lt;/li&gt;
&lt;/ul&gt;




&lt;figure&gt;

&lt;img src=&#34;./normalizing_flows_2.svg&#34; alt=&#34;&amp;lt;b&amp;gt;Figure 1.&amp;lt;/b&amp;gt; A differentiable transformation $T$ maps a D-dimentional vector $\mathbf{u}\in\mathbb{R}^{D}$ to a D-dimentional vector $\mathbf{x}\in\mathbb{R}^{D}$ via yellow arrow. This transformation has a differentiable inverse denoted by $T^{-1}$ that maps $\mathbf{x}$ to $\mathbf{u}$ via purple arrow. Jacobian matrices for these functions are depicted as squares, $J\_{T}(\mathbf{u})\in\mathbb{R}^{D\times D}$ and $J\_{T^{-1}}(\mathbf{x})\in\mathbb{R}^{D\times D}$.&#34; width=&#34;450&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;b&gt;Figure 1.&lt;/b&gt; A differentiable transformation $T$ maps a D-dimentional vector $\mathbf{u}\in\mathbb{R}^{D}$ to a D-dimentional vector $\mathbf{x}\in\mathbb{R}^{D}$ via yellow arrow. This transformation has a differentiable inverse denoted by $T^{-1}$ that maps $\mathbf{x}$ to $\mathbf{u}$ via purple arrow. Jacobian matrices for these functions are depicted as squares, $J_{T}(\mathbf{u})\in\mathbb{R}^{D\times D}$ and $J_{T^{-1}}(\mathbf{x})\in\mathbb{R}^{D\times D}$.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Change of variables&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;We can express probability density $p_{x}$ in terms of $p_{u}$ using change of variables:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$p_{x}(\mathbf{x})=p_{u}(\mathbf{u})\left|\frac{\partial\mathbf{u}}{\partial\mathbf{x}}\right|=p_{u}(\mathbf{u})\left|\frac{\partial}{\partial\mathbf{x}} T^{-1}(\mathbf{x})\right|=p_{u}(T^{-1}(\mathbf{x}))\left|\det{J_{T^{-1}}(\mathbf{x})}\right|$&lt;/li&gt;
&lt;li&gt;$p_{x}(\mathbf{x})=p_{u}(\mathbf{u})\left|\frac{\partial\mathbf{x}}{\partial\mathbf{u}}\right|^{-1}=p_{u}(\mathbf{u})\left|\frac{\partial}{\partial\mathbf{u}}T(\mathbf{u})\right|^{-1}=p_{u}(\mathbf{u})\left|\det{J_{T}(\mathbf{u})}\right|^{-1}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;given that the transformation $T$ is invertible and both $T$ and $T^{-1}$ are differentiable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Applications&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Sample from $p_{x}(\mathbf{x})$ by sampling points from $p_{u}(\mathbf{u})$, i.e. $\mathbf{x}=T(\mathbf{u})$ where $\mathbf{u} \sim p_{u}(\mathbf{u})$. This requires the ability to sample from $p_{u}(\mathbf{u})$ and to compute the transformation $T$.&lt;/li&gt;
&lt;li&gt;Evaluate model&amp;rsquo;s density using $p_{x}(\mathbf{x})=p_{u}(T^{-1}(\mathbf{x}))\left|\det{J_{T^{-1}}(\mathbf{x})}\right|$. This requires computing the inverse transformation $T^{-1}$ and its Jacobian determinant, and evaluating the density $p_{u}(\mathbf{u})$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Function composition&lt;/strong&gt;&lt;/p&gt;




&lt;figure&gt;

&lt;img src=&#34;./nf_3.svg&#34; alt=&#34;&amp;lt;b&amp;gt;Figure 2.&amp;lt;/b&amp;gt; A differentiable transformation $T=T\_3\circ T\_2\circ T\_1$ which is a compostion of three functions $T\_1, T\_2, T\_3$ maps a vector $\mathbf{u}\in\mathbb{R}^{D}$ to a vector $\mathbf{x}\in\mathbb{R}^{D}$ via yellow arrow. The inverse transformation $T^{-1}=T\_{1}^{-1}\circ T\_{2}^{-1}\circ T\_{3}^{-1}$ maps $\mathbf{u}$ to $\mathbf{x}$ via purple arrow. $\mathbf{z}\_{i}$ vectors denote in between states with $\mathbf{u}=\mathbf{z}\_{0}$ and $\mathbf{x}=\mathbf{z}\_{3}$.&#34; width=&#34;650&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  
  &lt;p&gt;
    &lt;b&gt;Figure 2.&lt;/b&gt; A differentiable transformation $T=T_3\circ T_2\circ T_1$ which is a compostion of three functions $T_1, T_2, T_3$ maps a vector $\mathbf{u}\in\mathbb{R}^{D}$ to a vector $\mathbf{x}\in\mathbb{R}^{D}$ via yellow arrow. The inverse transformation $T^{-1}=T_{1}^{-1}\circ T_{2}^{-1}\circ T_{3}^{-1}$ maps $\mathbf{u}$ to $\mathbf{x}$ via purple arrow. $\mathbf{z}_{i}$ vectors denote in between states with $\mathbf{u}=\mathbf{z}_{0}$ and $\mathbf{x}=\mathbf{z}_{3}$.
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Given three invertible and differentiable transformations $T_1, T_2$ and $T_3$ their composition $T=T_3 \circ T_2 \circ T_1$ is also invertible and differentiable:&lt;/p&gt;

&lt;p&gt;\begin{align}
T(\mathbf{u})&amp;amp;=T_3 \circ T_2 \circ T_1 (\mathbf{u}),\\&lt;br /&gt;
T^{-1}(\mathbf{x})&amp;amp;=(T_3 \circ T_2 \circ T_1)^{-1}(\mathbf{x}) = T_1^{-1} \circ T_2^{-1} \circ T_3^{-1}(\mathbf{x}),\\&lt;br /&gt;
\frac{\partial}{\partial \mathbf{u}}T(\mathbf{u})&amp;amp;=\frac{\partial}{\partial \mathbf{u}}T_3 \circ T_2 \circ T_1 (\mathbf{u})=\frac{\partial T_1(\mathbf{u})}{\partial \mathbf{u}}\frac{\partial T_2(T_1(\mathbf{u}))}{\partial T_1(\mathbf{u})}\frac{\partial T_3(T_2(T_1(\mathbf{u})))}{\partial T_2(T_1(\mathbf{u}))}=\frac{\partial T_1}{\partial \mathbf{u}}\frac{\partial T_2}{\partial T_1}\frac{\partial T_3}{\partial T_2},\\&lt;br /&gt;
\frac{\partial}{\partial \mathbf{x}}T^{-1}(\mathbf{x})&amp;amp;=\frac{\partial}{\partial \mathbf{x}}T_1^{-1} \circ T_2^{-1} \circ T_3^{-1} (\mathbf{x})=\frac{\partial T_3^{-1}(\mathbf{x})}{\partial \mathbf{x}}\frac{\partial T_2^{-1}(T_3^{-1}(\mathbf{x}))}{\partial T_3^{-1}(\mathbf{x})}\frac{\partial T_1^{-1}(T_2^{-1}(T_3^{-1}(\mathbf{x})))}{\partial T_2^{-1}(T_3^{-1}(\mathbf{x}))}=\frac{\partial T_3^{-1}}{\partial \mathbf{x}}\frac{\partial T_2^{-1}}{\partial T_3^{-1}}\frac{\partial T_1^{-1}}{\partial T_2^{-1}},
\end{align}
where for differentiation the chain rule was applied. Now to calculate the Jacobian matrix of this transformation we can use the fact that the determinant of a matrix product of square matrices equals the product of their determinants, i.e. $\det{AB}=\det{A}\cdot\det{B}$ where $A, B\in\mathbb{R}^{D\times D}$.&lt;/p&gt;

&lt;p&gt;\begin{align}
\det{\frac{\partial}{\partial \mathbf{u}}T(\mathbf{u})}=&amp;amp;\det{J_{T_3\circ T_2 \circ T_1}(\mathbf{u})}=\det{\frac{\partial T_1}{\partial \mathbf{u}}\frac{\partial T_2}{\partial T_1}\frac{\partial T_3}{\partial T_2}}=\det{J_{T_{1}}(\mathbf{u})}\cdot\det{J_{T_2}(T_{1}(\mathbf{u}))}\cdot\det{J_{T_3}(T_{2}(T_{1}(\mathbf{u})))},\\&lt;br /&gt;
\det{\frac{\partial}{\partial \mathbf{x}}T^{-1}(\mathbf{x})}&amp;amp;=\det{J_{T_1^{-1} \circ T_2^{-1}\circ T_3^{-1}}(\mathbf{x})}=\det{\frac{\partial T_3^{-1}}{\partial \mathbf{x}}\frac{\partial T_2^{-1}}{\partial T_3^{-1}}\frac{\partial T_1^{-1}}{\partial T_2^{-1}}}=\det{J_{T_{3}^{-1}}(\mathbf{x})}\cdot\det{J_{T_2^{-1}}(T_{3}^{-1}(\mathbf{x}))}\cdot\det{J_{T_1^{-1}}(T_2^{-1}(T_{3}^{-1}(\mathbf{x})))}.
\end{align}&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Minimizing loss of log likelihood&lt;/strong&gt;
\begin{align}
\mathcal{L}(\boldsymbol{\theta})&amp;amp;=-\mathbb{E}_{p_{x}^{*}(\mathbf{x})}\left[\log{p_{x}(\mathbf{x};\boldsymbol{\theta})}\right]\\&lt;br /&gt;
&amp;amp;=-\mathbb{E}_{p_{x}^{*}(\mathbf{x})}\left[\log{p_{u}(T^{-1}(\mathbf{x}; \boldsymbol{\phi});\boldsymbol{\psi})}+\log{\left|\det{J_{T^{-1}}(\mathbf{x}; \boldsymbol{\phi})}\right|}\right]\\&lt;br /&gt;
&amp;amp;\approx-\frac{1}{N}\sum_{n=1}^{N}\log{p_{u}(T^{-1}(\mathbf{x}_{n}; \boldsymbol{\phi});\boldsymbol{\psi})}+\log{\left|\det{J_{T^{-1}}(\mathbf{x}_{n}; \boldsymbol{\phi})}\right|}
\end{align}&lt;/p&gt;

&lt;!-- **Appendix**


* $p\_{u}(\mathbf{u})=p\_{x}(\mathbf{x})\left|\frac{\partial\mathbf{x}}{\partial\mathbf{u}}\right|=p\_{x}(\mathbf{x})\left|\frac{\partial}{\partial\mathbf{u}} T(\mathbf{u})\right|=p\_{x}(T(\mathbf{u}))\left|\det{J\_{T}(\mathbf{u})}\right|$
* $p\_{u}(\mathbf{u})=p\_{x}(\mathbf{x})\left|\frac{\partial\mathbf{u}}{\partial\mathbf{x}}\right|^{-1}=p\_{x}(\mathbf{x})\left|\frac{\partial}{\partial\mathbf{x}}T^{-1}(\mathbf{x})\right|^{-1}=p\_{x}(\mathbf{x})\left|\det{J\_{T^{-1}}(\mathbf{x})}\right|^{-1}$

**Function composition**

Given two invertible and differentiable transformations $T\_1$ and $T\_2$ their composition $T=T\_2 \circ T\_1$ is also invertible and differentiable:

\begin{align}
\label{eq:sample}\tag{1}
T^{-1}&amp;=(T\_2 \circ T\_1)^{-1} = T\_1^{-1} \circ T\_2^{-1},\\\\\\
\tag{2}
\det{J\_{T\_2 \circ T\_1}(\mathbf{u})}&amp;=\det{\frac{\partial}{\partial \mathbf{u}}{T\_2 \circ T\_1}(\mathbf{u})}=\det{\frac{\partial T\_{1}(\mathbf{u})}{\partial \mathbf{u}}\frac{\partial}{\partial T\_{1}(\mathbf{u})}{T\_2 \circ T\_1}(\mathbf{u})}=\det{J\_{T\_{1}}(\mathbf{u})}\cdot\det{J\_{T\_2}(T\_{1}(\mathbf{u}))},\\\\\\
\tag{3}
\det{J\_{T\_1^{-1} \circ T\_2^{-1}}(\mathbf{x})}&amp;=\det{\frac{\partial}{\partial \mathbf{x}}{T\_1^{-1} \circ T\_2^{-1}}(\mathbf{u})}=\det{\frac{\partial T\_{2}^{-1}(\mathbf{x})}{\partial \mathbf{x}}\frac{\partial}{\partial T\_{2}^{-1}(\mathbf{x})}{T\_1^{-1} \circ T\_2^{-1}}(\mathbf{x})}=\det{J\_{T\_{2}^{-1}}(\mathbf{x})}\cdot\det{J\_{T\_1^{-1}}(T\_{2}^{-1}(\mathbf{x}))}
\end{align} --&gt;
</description>
    </item>
    
    <item>
      <title>Theoretical Physics</title>
      <link>https://dauparas.github.io/links/physics/</link>
      <pubDate>Fri, 29 Mar 2019 11:26:20 -0400</pubDate>
      
      <guid>https://dauparas.github.io/links/physics/</guid>
      <description>&lt;p&gt;This is a link to theoretical physics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unified framework for multivariate distributions in biological sequences</title>
      <link>https://dauparas.github.io/publication/unified/</link>
      <pubDate>Sun, 03 Mar 2019 20:01:19 -0500</pubDate>
      
      <guid>https://dauparas.github.io/publication/unified/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Depth and nonlinearity induce implicit exploration</title>
      <link>https://dauparas.github.io/publication/rl/</link>
      <pubDate>Tue, 29 May 2018 19:18:17 -0500</pubDate>
      
      <guid>https://dauparas.github.io/publication/rl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-organization of swimmers drives long-range fluid transport in bacterial colonies</title>
      <link>https://dauparas.github.io/publication/self_organisation/</link>
      <pubDate>Sat, 24 Feb 2018 20:25:06 -0500</pubDate>
      
      <guid>https://dauparas.github.io/publication/self_organisation/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://dauparas.github.io/publication/corner_flows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dauparas.github.io/publication/corner_flows/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://dauparas.github.io/publication/flagellar_flows/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dauparas.github.io/publication/flagellar_flows/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://dauparas.github.io/publication/helical_micropumps/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dauparas.github.io/publication/helical_micropumps/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://dauparas.github.io/publication/thesis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dauparas.github.io/publication/thesis/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Theoretical Physics</title>
      <link>https://dauparas.github.io/talk/physics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://dauparas.github.io/talk/physics/</guid>
      <description>&lt;p&gt;This is a link to theoretical physics.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
